{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTMS1-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT(Bidirectional Encoder Representations  from Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1810.04805\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104185ba8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1810.04805', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "### 为什么说是Bidirectional\n",
    "https://blog.csdn.net/laobai1015/article/details/87937528\n",
    "\n",
    "BERT：全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息，这种“双向”的来源在于BERT与传统语言模型不同，其中 BERT 和 ELMo 都使用双向信息，OpenAI GPT 使用单向信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "https://zhuanlan.zhihu.com/p/74090249\n",
    "\n",
    "bert的train包含两个过程。\n",
    "Pre-training & fine-tuning\n",
    "\n",
    "![](img/bert01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT的输入\n",
    "reference: microstrong\n",
    "\n",
    "bert的输入包含3个部分: \n",
    "* Token Embedding, \n",
    "* Segment Embedding, 标记token是属于句子A还是句子B\n",
    "* Positon Embedding。\n",
    "\n",
    "最后把这三个Embedding的对应位置加起来，作为BERT最后的输入Embedding\n",
    "### 特殊字符介绍\n",
    "CLS bert中编码 101\n",
    "\n",
    "SEP bert中编码 102\n",
    "\n",
    "UNK bert中编码 100\n",
    "\n",
    "PAD bert中编码 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training: How to train a new language model from scratch using Transformers and Tokenizers\n",
    "\n",
    "https://huggingface.co/blog/how-to-train?nsukey=MwfrrZHYtrS9g%2F2Y3hxHCiUr6QiHNgZ9Nb%2BhPS2oFosDP0vUdsyh8Nrs%2F7sc7%2FPEN3yYaxo%2BNJQtoe%2BR1hZc%2BNf4hnknnCpCDzioGByvE5F6Zen4MoyyFWGNioRFeUDCpqDzr8DEbQL0bI1%2B4QGie1nCT2PeplBJKRi9IAd8DSfx64yFkZlstBx%2FAcFNr6ky8j3RbAKXkzaCulH5I3TWiA%3D%3D\n",
    "\n",
    "https://github.com/huggingface/blog/tree/master/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd =\"\"\"\n",
    "python run_language_modeling.py\n",
    "    --train_data_file ./oscar.eo.txt\n",
    "    --output_dir ./EsperBERTo-small-v1\n",
    "    --model_type roberta\n",
    "    --mlm\n",
    "    --config_name ./EsperBERTo\n",
    "    --tokenizer_name ./EsperBERTo\n",
    "    --do_train\n",
    "    --line_by_line\n",
    "    --learning_rate 1e-4\n",
    "    --num_train_epochs 1\n",
    "    --save_total_limit 2\n",
    "    --save_steps 2000\n",
    "    --per_gpu_train_batch_size 16\n",
    "    --seed 42\n",
    "\"\"\".replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_language_modeling.py\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet).\n",
    "\n",
    "\n",
    "GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. \n",
    "\n",
    "BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. \n",
    "\n",
    "XLNet is fine-tuned using a permutation language modeling (PLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "#     DataCollatorForPermutationLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
    "    )\n",
    "    eval_data_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "\n",
    "    mlm: bool = field(\n",
    "        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    plm_probability: float = field(\n",
    "        default=1 / 6,\n",
    "        metadata={\n",
    "            \"help\": \"Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\"\n",
    "        },\n",
    "    )\n",
    "    max_span_length: int = field(\n",
    "        default=5, metadata={\"help\": \"Maximum length of a span of masked tokens for permutation language modeling.\"}\n",
    "    )\n",
    "\n",
    "    block_size: int = field(\n",
    "        default=-1,\n",
    "        metadata={\n",
    "            \"help\": \"Optional input sequence length after tokenization.\"\n",
    "            \"The training dataset will be truncated in block of this size for training.\"\n",
    "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset(args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False):\n",
    "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
    "    if args.line_by_line:\n",
    "        return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\n",
    "    else:\n",
    "        return TextDataset(\n",
    "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size, overwrite_cache=args.overwrite_cache\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    if data_args.eval_data_file is None and training_args.do_eval:\n",
    "        raise ValueError(\n",
    "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "            \"or remove the --do_eval argument.\"\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    if model_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n",
    "    elif model_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "            \"and load it from here, using --tokenizer_name\"\n",
    "        )\n",
    "\n",
    "    if model_args.model_name_or_path:\n",
    "        model = AutoModelWithLMHead.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelWithLMHead.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if config.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not data_args.mlm:\n",
    "        raise ValueError(\n",
    "            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the\"\n",
    "            \"--mlm flag (masked language modeling).\"\n",
    "        )\n",
    "\n",
    "    if data_args.block_size <= 0:\n",
    "        data_args.block_size = tokenizer.max_len\n",
    "        # Our input block size will be the max possible for the model\n",
    "    else:\n",
    "        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n",
    "\n",
    "    # Get datasets\n",
    "\n",
    "    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
    "    eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
    "    if config.model_type == \"xlnet\":\n",
    "#         data_collator = DataCollatorForPermutationLanguageModeling(\n",
    "#             tokenizer=tokenizer, plm_probability=data_args.plm_probability, max_span_length=data_args.max_span_length,\n",
    "#         )\n",
    "        pass\n",
    "    else:\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n",
    "        )\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        model_path = (\n",
    "            model_args.model_name_or_path\n",
    "            if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
    "            else None\n",
    "        )\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "        result = {\"perplexity\": perplexity}\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(\"***** Eval results *****\")\n",
    "                for key in sorted(result.keys()):\n",
    "                    logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert的变种\n",
    "https://huggingface.co/transformers/model_summary.html\n",
    "\n",
    "## 改进思路\n",
    "https://medium.com/analytics-vidhya/what-happens-after-bert-summarize-those-ideas-behind-ee02f1eae5d9\n",
    "\n",
    "\n",
    "### Increase coverage to improve MaskedLM\n",
    "Masking on whole word —wwm\n",
    "Masking on Phrase level — ERNIE\n",
    "Scaling to a certain length — Ngram Masking / Span Masking\n",
    "\n",
    "Phrase level needs to provide a corresponding phrase list. Providing such artificially added messages may disturb the model, give it a bias. It seems that mask on longer length should be a better solution, so T5 try on different lengths to reach this conclusion:\n",
    "\n",
    "It can be seen that increasing the length is effective, but it does not mean that longer is better. SpanBert has a better solution, to reduce the chance of Mask overly long text through probability sampling.\n",
    "\n",
    "### Change the proportion of Masked\n",
    "Google’s T5 tries different masked ratios to explore what the best parameter settings are.Surprisingly, bert original setting is the best :\n",
    "\n",
    "\n",
    "### NextSentencePrediction 👎?\n",
    "NSP learns sentence-level information by predicting whether two sentences are contextual. From the experimental result, it didn’t give much improvement, and even drops on some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT\n",
    "Same as BERT but with a few tweaks:\n",
    "\n",
    "        Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token) whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it’s more logical to have H >> E. Als, the embedding matrix is large since it’s V x E (V being the vocab size). If E < H, it has less parameters.\n",
    "\n",
    "        Layers are split in groups that share parameters (to save memory).\n",
    "\n",
    "        Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A et B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1909.11942\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1107f5b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1909.11942', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point fur-ther model increases become harder due to GPU/TPU memory limitations and longer  training  times.   \n",
    "\n",
    "To  address  these  problems,  we  present  two  parameter-reduction  techniques  to  lower  memory  consumption  and  increase  the  training speed of BERT (Devlin et al., 2019).  Comprehensive empirical evidence shows that  our  proposed  methods  lead  to  models  that  scale  much  better  compared  to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa: A Robustly Optimized BERT Pretraining Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1907.11692\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1064a6048>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1907.11692', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Language model pretraining has led to sig-nificant performance gains but careful com-parison between different approaches is chal-lenging. Training is computationally expen-sive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final re-sults. We present a replication study of BERT pretraining (Devlin et al.,2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.  Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n",
    "\n",
    "\n",
    "Our modifications are simple, they include: \n",
    "* (1)training the model longer, with bigger batches,over more data; \n",
    "* (2) removing the next sentenceprediction objective; \n",
    "* (3) training on longer se-quences; and \n",
    "* (4) dynamically changing the mask-ing pattern applied to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与BERT的比较\n",
    "\n",
    "Same as BERT with better pretraining tricks:\n",
    "\n",
    "        dynamic masking: tokens are masked differently at each epoch whereas BERT does it once and for all\n",
    "\n",
    "        no NSP (next sentence prediction) loss and instead of putting just two sentences together, put a chunk of contiguous texts together to reach 512 tokens (so sentences in in an order than may span other several documents)\n",
    "\n",
    "        train with larger batches\n",
    "\n",
    "        use BPE with bytes as a subunit and not characters (because of unicode characters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "Same as BERT but smaller. Trained by distillation of the pretrained BERT model, meaning it’s been trained to predict the same probabilities as the larger model. The actual objective is a combination of:\n",
    "\n",
    "        finding the same probabilities as the teacher model\n",
    "\n",
    "        predicting the masked tokens correctly (but no next-sentence objective)\n",
    "\n",
    "        a cosine similarity between the hidden states of the student and the teacher model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型蒸馏Distillation\n",
    "https://zhuanlan.zhihu.com/p/71986772\n",
    "\n",
    "Hinton在NIPS2014[1]提出了知识蒸馏（Knowledge Distillation）的概念，旨在把一个大模型或者多个模型ensemble学到的知识迁移到另一个轻量级单模型上，方便部署。简单的说就是用新的小模型去学习大模型的预测结果，改变一下目标函数。听起来是不难，但在实践中小模型真的能拟合那么好吗？所以还是要多看看别人家的实验，掌握一些trick。\n",
    "### 名词解释\n",
    "\n",
    "    teacher - 原始模型或模型ensemble\n",
    "    student - 新模型\n",
    "    transfer set - 用来迁移teacher知识、训练student的数据集合\n",
    "    soft target - teacher输出的预测结果（一般是softmax之后的概率）\n",
    "    hard target - 样本原本的标签\n",
    "    temperature - 蒸馏目标函数中的超参数\n",
    "    born-again network - 蒸馏的一种，指student和teacher的结构和尺寸完全一样\n",
    "    teacher annealing - 防止student的表现被teacher限制，在蒸馏时逐渐减少soft targets的权重\n",
    "\n",
    "### 基本思想\n",
    "1.1 为什么蒸馏可以work\n",
    "\n",
    "好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据。所以蒸馏的目标是让student学习到teacher的泛化能力，理论上得到的结果会比单纯拟合训练数据的student要好。另外，对于分类任务，如果soft targets的熵比hard targets高，那显然student会学习到更多的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# domain-adaptive  pretraining\n",
    "https://mp.weixin.qq.com/s/qULq9ye_Pg56pEQIdvr8tQ\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/149210123\n",
    "\n",
    "ACL2020 Best Paper有一篇论文提名奖，《Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks》。这篇论文做了很多语言模型预训练的实验，系统的分析了语言模型预训练对子任务的效果提升情况。有几个主要结论：\n",
    "\n",
    "在目标领域的数据集上继续预训练（DAPT）可以提升效果；目标领域的语料与RoBERTa的原始预训练语料越不相关，DAPT效果则提升更明显。\n",
    "\n",
    "在具体任务的数据集上继续预训练（TAPT）可以十分“廉价”地提升效果。\n",
    "\n",
    "结合二者（先进行DAPT，再进行TAPT）可以进一步提升效果。\n",
    "\n",
    "如果能获取更多的、任务相关的无标注数据继续预训练（Curated-TAPT），效果则最佳。\n",
    "\n",
    "如果无法获取更多的、任务相关的无标注数据，采取一种十分轻量化的简单数据选择策略，效果也会提升。\n",
    "\n",
    "为了更好地理解这篇paper，我们需要牢记两个重要的专有名词：\n",
    "\n",
    "* DAPT：领域自适应预训练(Domain-Adaptive Pretraining)，就是在所属的领域（如医疗）数据上继续预训练～\n",
    "* TAPT：任务自适应预训练(Task-Adaptive Pretraining)，就是在具体任务数据上继续预训练～"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/2004.10964\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1071b2f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/2004.10964', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Language  models  pretrained  on  text  from  a wide  variety  of  sources  form  the  foundation of  today’s  NLP.  In  light  of  the  success  of these  broad-coverage  models,  we  investigate whether it is still helpful to tail or a pretrained model  to  the  domain  of  a  target  task.    \n",
    "\n",
    "We present a study across four domains (\n",
    "* biomedical and \n",
    "* computer science publications,  \n",
    "* news,and  \n",
    "* reviews)  and  \n",
    "\n",
    "eight  classification  tasks,showing that a second phase of pretraining in-domain  (domain-adaptive  pretraining)  leads to  performance  gains,  under  both  high- and low-resource  settings.\n",
    "\n",
    "Moreover,   adapting to  the  task’s  unlabeled  data  (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. \n",
    "\n",
    "Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable.  Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch transformers 预训练模型\n",
    "https://mp.weixin.qq.com/s/qULq9ye_Pg56pEQIdvr8tQ\n",
    "\n",
    "虽然在bert上语言模型预训练在算法比赛中已经是一个稳定的上分操作。但是上面这篇文章难能可贵的是对这个操作进行了系统分析。大部分中文语言模型都是在tensorflow上训练的，一个常见例子是中文roberta项目。可以参考\n",
    "\n",
    "https://github.com/brightmart/roberta_zh\n",
    "\n",
    "\n",
    "\n",
    "使用pytorch进行中文bert语言模型预训练的例子比较少。在huggingface的Transformers中，有一部分代码支持语言模型预训练(不是很丰富，很多功能都不支持比如wwm)。为了用最少的代码成本完成bert语言模型预训练，本文借鉴了里面的一些现成代码。也尝试分享一下使用pytorch进行语言模型预训练的一些经验。主要有三个常见的中文bert语言模型\n",
    "\n",
    "bert-base-chinese\n",
    "\n",
    "roberta-wwm-ext\n",
    "\n",
    "ernie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-chinese\n",
    "\n",
    "\n",
    "\n",
    "(https://huggingface.co/bert-base-chinese)\n",
    "\n",
    "这是最常见的中文bert语言模型，基于中文维基百科相关语料进行预训练。把它作为baseline，在领域内无监督数据进行语言模型预训练很简单。只需要使用官方给的例子就好。\n",
    "\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling\n",
    "\n",
    "(本文使用的transformers更新到3.0.2)\n",
    "\n",
    "方法就是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=bert \\\n",
    "    --model_name_or_path=bert-base-chinese \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$TEST_FILE \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$TRAIN_FILE 代表领域相关中文语料地址。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roberta-wwm-ext\n",
    "\n",
    "(https://github.com/ymcui/Chinese-BERT-wwm)\n",
    "\n",
    "哈工大讯飞联合实验室发布的预训练语言模型。预训练的方式是采用roberta类似的方法，比如动态mask，更多的训练数据等等。在很多任务中，该模型效果要优于bert-base-chinese。\n",
    "\n",
    "对于中文roberta类的pytorch模型，使用方法如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "roberta = BertModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切记不可使用官方推荐的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'chinese-roberta-wwm-ext'  # 预训练模型保存目录 \n",
    "model.save_pretrained(dir_)  # 会生成 模型bin文件和config.json\n",
    "tokenizer.save_pretrained(dir_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为中文roberta类的配置文件比如vocab.txt，都是采用bert的方法设计的。英文roberta模型读取配置文件的格式默认是vocab.json。对于一些英文roberta模型，倒是可以通过AutoModel自动读取。这就解释了huggingface的模型库的中文roberta示例代码为什么跑不通。https://huggingface.co/models?\n",
    "\n",
    "如果要基于上面的代码run_language_modeling.py继续预训练roberta。还需要做两个改动。\n",
    "\n",
    "下载roberta-wwm-ext到本地目录hflroberta，在config.json中修改“model_type”:\"roberta\"为\"model_type\":\"bert\"。\n",
    "\n",
    "对上面的run_language_modeling.py中的AutoModel和AutoTokenizer都进行替换为BertModel和BertTokenizer。\n",
    "\n",
    "再运行命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_language_modeling_roberta.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=bert \\\n",
    "    --model_name_or_path=hflroberta \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$TEST_FILE \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ernie\n",
    "\n",
    "（https://github.com/nghuyong/ERNIE-Pytorch ）\n",
    "\n",
    "ernie是百度发布的基于百度知道贴吧等中文语料结合实体预测等任务生成的预训练模型。这个模型的准确率在某些任务上要优于bert-base-chinese和roberta。如果基于ernie1.0模型做领域数据预训练的话只需要一步修改。\n",
    "\n",
    "下载ernie1.0到本地目录ernie，在config.json中增加字段\"model_type\":\"bert\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=bert \\\n",
    "    --model_name_or_path=ernie \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$TEST_FILE \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，huggingface项目中语言模型预训练用mask方式如下。仍是按照15%的数据随机mask然后预测自身。如果要做一些高级操作比如whole word masking或者实体预测，可以自行修改transformers.DataCollatorForLanguageModeling。\n",
    "\n",
    "本文实验代码库。拿来即用！\n",
    "\n",
    "https://github.com/zhusleep/pytorch_chinese_lm_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
