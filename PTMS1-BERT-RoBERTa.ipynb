{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTMS1-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT(Bidirectional Encoder Representations  from Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1810.04805\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104185ba8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1810.04805', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "### ä¸ºä»€ä¹ˆè¯´æ˜¯Bidirectional\n",
    "https://blog.csdn.net/laobai1015/article/details/87937528\n",
    "\n",
    "BERTï¼šå…¨ç§°æ˜¯Bidirectional Encoder Representation from Transformersï¼Œå³åŒå‘Transformerçš„Encoderã€‚å…¶ä¸­â€œåŒå‘â€è¡¨ç¤ºæ¨¡å‹åœ¨å¤„ç†æŸä¸€ä¸ªè¯æ—¶ï¼Œå®ƒèƒ½åŒæ—¶åˆ©ç”¨å‰é¢çš„è¯å’Œåé¢çš„è¯ä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼Œè¿™ç§â€œåŒå‘â€çš„æ¥æºåœ¨äºBERTä¸ä¼ ç»Ÿè¯­è¨€æ¨¡å‹ä¸åŒï¼Œå…¶ä¸­ BERT å’Œ ELMo éƒ½ä½¿ç”¨åŒå‘ä¿¡æ¯ï¼ŒOpenAI GPT ä½¿ç”¨å•å‘ä¿¡æ¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "https://zhuanlan.zhihu.com/p/74090249\n",
    "\n",
    "bertçš„trainåŒ…å«ä¸¤ä¸ªè¿‡ç¨‹ã€‚\n",
    "Pre-training & fine-tuning\n",
    "\n",
    "![](img/bert01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTçš„è¾“å…¥\n",
    "reference: microstrong\n",
    "\n",
    "bertçš„è¾“å…¥åŒ…å«3ä¸ªéƒ¨åˆ†: \n",
    "* Token Embedding, \n",
    "* Segment Embedding, æ ‡è®°tokenæ˜¯å±äºå¥å­Aè¿˜æ˜¯å¥å­B\n",
    "* Positon Embeddingã€‚\n",
    "\n",
    "æœ€åæŠŠè¿™ä¸‰ä¸ªEmbeddingçš„å¯¹åº”ä½ç½®åŠ èµ·æ¥ï¼Œä½œä¸ºBERTæœ€åçš„è¾“å…¥Embedding\n",
    "### ç‰¹æ®Šå­—ç¬¦ä»‹ç»\n",
    "CLS bertä¸­ç¼–ç  101\n",
    "\n",
    "SEP bertä¸­ç¼–ç  102\n",
    "\n",
    "UNK bertä¸­ç¼–ç  100\n",
    "\n",
    "PAD bertä¸­ç¼–ç  0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training: How to train a new language model from scratch using Transformers and Tokenizers\n",
    "\n",
    "https://huggingface.co/blog/how-to-train?nsukey=MwfrrZHYtrS9g%2F2Y3hxHCiUr6QiHNgZ9Nb%2BhPS2oFosDP0vUdsyh8Nrs%2F7sc7%2FPEN3yYaxo%2BNJQtoe%2BR1hZc%2BNf4hnknnCpCDzioGByvE5F6Zen4MoyyFWGNioRFeUDCpqDzr8DEbQL0bI1%2B4QGie1nCT2PeplBJKRi9IAd8DSfx64yFkZlstBx%2FAcFNr6ky8j3RbAKXkzaCulH5I3TWiA%3D%3D\n",
    "\n",
    "https://github.com/huggingface/blog/tree/master/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd =\"\"\"\n",
    "python run_language_modeling.py\n",
    "    --train_data_file ./oscar.eo.txt\n",
    "    --output_dir ./EsperBERTo-small-v1\n",
    "    --model_type roberta\n",
    "    --mlm\n",
    "    --config_name ./EsperBERTo\n",
    "    --tokenizer_name ./EsperBERTo\n",
    "    --do_train\n",
    "    --line_by_line\n",
    "    --learning_rate 1e-4\n",
    "    --num_train_epochs 1\n",
    "    --save_total_limit 2\n",
    "    --save_steps 2000\n",
    "    --per_gpu_train_batch_size 16\n",
    "    --seed 42\n",
    "\"\"\".replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_language_modeling.py\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet).\n",
    "\n",
    "\n",
    "GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. \n",
    "\n",
    "BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. \n",
    "\n",
    "XLNet is fine-tuned using a permutation language modeling (PLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "#     DataCollatorForPermutationLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
    "    )\n",
    "    eval_data_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "\n",
    "    mlm: bool = field(\n",
    "        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    plm_probability: float = field(\n",
    "        default=1 / 6,\n",
    "        metadata={\n",
    "            \"help\": \"Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.\"\n",
    "        },\n",
    "    )\n",
    "    max_span_length: int = field(\n",
    "        default=5, metadata={\"help\": \"Maximum length of a span of masked tokens for permutation language modeling.\"}\n",
    "    )\n",
    "\n",
    "    block_size: int = field(\n",
    "        default=-1,\n",
    "        metadata={\n",
    "            \"help\": \"Optional input sequence length after tokenization.\"\n",
    "            \"The training dataset will be truncated in block of this size for training.\"\n",
    "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataset(args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False):\n",
    "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
    "    if args.line_by_line:\n",
    "        return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\n",
    "    else:\n",
    "        return TextDataset(\n",
    "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size, overwrite_cache=args.overwrite_cache\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    if data_args.eval_data_file is None and training_args.do_eval:\n",
    "        raise ValueError(\n",
    "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "            \"or remove the --do_eval argument.\"\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    if model_args.config_name:\n",
    "        config = AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n",
    "    elif model_args.model_name_or_path:\n",
    "        config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "    else:\n",
    "        config = CONFIG_MAPPING[model_args.model_type]()\n",
    "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
    "            \"and load it from here, using --tokenizer_name\"\n",
    "        )\n",
    "\n",
    "    if model_args.model_name_or_path:\n",
    "        model = AutoModelWithLMHead.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Training new model from scratch\")\n",
    "        model = AutoModelWithLMHead.from_config(config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if config.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not data_args.mlm:\n",
    "        raise ValueError(\n",
    "            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the\"\n",
    "            \"--mlm flag (masked language modeling).\"\n",
    "        )\n",
    "\n",
    "    if data_args.block_size <= 0:\n",
    "        data_args.block_size = tokenizer.max_len\n",
    "        # Our input block size will be the max possible for the model\n",
    "    else:\n",
    "        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n",
    "\n",
    "    # Get datasets\n",
    "\n",
    "    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
    "    eval_dataset = get_dataset(data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
    "    if config.model_type == \"xlnet\":\n",
    "#         data_collator = DataCollatorForPermutationLanguageModeling(\n",
    "#             tokenizer=tokenizer, plm_probability=data_args.plm_probability, max_span_length=data_args.max_span_length,\n",
    "#         )\n",
    "        pass\n",
    "    else:\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n",
    "        )\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        model_path = (\n",
    "            model_args.model_name_or_path\n",
    "            if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n",
    "            else None\n",
    "        )\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_master():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "\n",
    "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "        result = {\"perplexity\": perplexity}\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
    "        if trainer.is_world_master():\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                logger.info(\"***** Eval results *****\")\n",
    "                for key in sorted(result.keys()):\n",
    "                    logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bertçš„å˜ç§\n",
    "https://huggingface.co/transformers/model_summary.html\n",
    "\n",
    "## æ”¹è¿›æ€è·¯\n",
    "https://medium.com/analytics-vidhya/what-happens-after-bert-summarize-those-ideas-behind-ee02f1eae5d9\n",
    "\n",
    "\n",
    "### Increase coverage to improve MaskedLM\n",
    "Masking on whole word â€”wwm\n",
    "Masking on Phrase level â€” ERNIE\n",
    "Scaling to a certain length â€” Ngram Masking / Span Masking\n",
    "\n",
    "Phrase level needs to provide a corresponding phrase list. Providing such artificially added messages may disturb the model, give it a bias. It seems that mask on longer length should be a better solution, so T5 try on different lengths to reach this conclusion:\n",
    "\n",
    "It can be seen that increasing the length is effective, but it does not mean that longer is better. SpanBert has a better solution, to reduce the chance of Mask overly long text through probability sampling.\n",
    "\n",
    "### Change the proportion of Masked\n",
    "Googleâ€™s T5 tries different masked ratios to explore what the best parameter settings are.Surprisingly, bert original setting is the best :\n",
    "\n",
    "\n",
    "### NextSentencePrediction ğŸ‘?\n",
    "NSP learns sentence-level information by predicting whether two sentences are contextual. From the experimental result, it didnâ€™t give much improvement, and even drops on some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT\n",
    "Same as BERT but with a few tweaks:\n",
    "\n",
    "        Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token) whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so itâ€™s more logical to have H >> E. Als, the embedding matrix is large since itâ€™s V x E (V being the vocab size). If E < H, it has less parameters.\n",
    "\n",
    "        Layers are split in groups that share parameters (to save memory).\n",
    "\n",
    "        Next sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A et B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1909.11942\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1107f5b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1909.11942', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point fur-ther model increases become harder due to GPU/TPU memory limitations and longer  training  times.   \n",
    "\n",
    "To  address  these  problems,  we  present  two  parameter-reduction  techniques  to  lower  memory  consumption  and  increase  the  training speed of BERT (Devlin et al., 2019).  Comprehensive empirical evidence shows that  our  proposed  methods  lead  to  models  that  scale  much  better  compared  to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa: A Robustly Optimized BERT Pretraining Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1907.11692\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1064a6048>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1907.11692', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Language model pretraining has led to sig-nificant performance gains but careful com-parison between different approaches is chal-lenging. Training is computationally expen-sive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final re-sults. We present a replication study of BERT pretraining (Devlin et al.,2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.  Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n",
    "\n",
    "\n",
    "Our modifications are simple, they include: \n",
    "* (1)training the model longer, with bigger batches,over more data; \n",
    "* (2) removing the next sentenceprediction objective; \n",
    "* (3) training on longer se-quences; and \n",
    "* (4) dynamically changing the mask-ing pattern applied to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸BERTçš„æ¯”è¾ƒ\n",
    "\n",
    "Same as BERT with better pretraining tricks:\n",
    "\n",
    "        dynamic masking: tokens are masked differently at each epoch whereas BERT does it once and for all\n",
    "\n",
    "        no NSP (next sentence prediction) loss and instead of putting just two sentences together, put a chunk of contiguous texts together to reach 512 tokens (so sentences in in an order than may span other several documents)\n",
    "\n",
    "        train with larger batches\n",
    "\n",
    "        use BPE with bytes as a subunit and not characters (because of unicode characters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "Same as BERT but smaller. Trained by distillation of the pretrained BERT model, meaning itâ€™s been trained to predict the same probabilities as the larger model. The actual objective is a combination of:\n",
    "\n",
    "        finding the same probabilities as the teacher model\n",
    "\n",
    "        predicting the masked tokens correctly (but no next-sentence objective)\n",
    "\n",
    "        a cosine similarity between the hidden states of the student and the teacher model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è’¸é¦Distillation\n",
    "https://zhuanlan.zhihu.com/p/71986772\n",
    "\n",
    "Hintonåœ¨NIPS2014[1]æå‡ºäº†çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰çš„æ¦‚å¿µï¼Œæ—¨åœ¨æŠŠä¸€ä¸ªå¤§æ¨¡å‹æˆ–è€…å¤šä¸ªæ¨¡å‹ensembleå­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°å¦ä¸€ä¸ªè½»é‡çº§å•æ¨¡å‹ä¸Šï¼Œæ–¹ä¾¿éƒ¨ç½²ã€‚ç®€å•çš„è¯´å°±æ˜¯ç”¨æ–°çš„å°æ¨¡å‹å»å­¦ä¹ å¤§æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œæ”¹å˜ä¸€ä¸‹ç›®æ ‡å‡½æ•°ã€‚å¬èµ·æ¥æ˜¯ä¸éš¾ï¼Œä½†åœ¨å®è·µä¸­å°æ¨¡å‹çœŸçš„èƒ½æ‹Ÿåˆé‚£ä¹ˆå¥½å—ï¼Ÿæ‰€ä»¥è¿˜æ˜¯è¦å¤šçœ‹çœ‹åˆ«äººå®¶çš„å®éªŒï¼ŒæŒæ¡ä¸€äº›trickã€‚\n",
    "### åè¯è§£é‡Š\n",
    "\n",
    "    teacher - åŸå§‹æ¨¡å‹æˆ–æ¨¡å‹ensemble\n",
    "    student - æ–°æ¨¡å‹\n",
    "    transfer set - ç”¨æ¥è¿ç§»teacherçŸ¥è¯†ã€è®­ç»ƒstudentçš„æ•°æ®é›†åˆ\n",
    "    soft target - teacherè¾“å‡ºçš„é¢„æµ‹ç»“æœï¼ˆä¸€èˆ¬æ˜¯softmaxä¹‹åçš„æ¦‚ç‡ï¼‰\n",
    "    hard target - æ ·æœ¬åŸæœ¬çš„æ ‡ç­¾\n",
    "    temperature - è’¸é¦ç›®æ ‡å‡½æ•°ä¸­çš„è¶…å‚æ•°\n",
    "    born-again network - è’¸é¦çš„ä¸€ç§ï¼ŒæŒ‡studentå’Œteacherçš„ç»“æ„å’Œå°ºå¯¸å®Œå…¨ä¸€æ ·\n",
    "    teacher annealing - é˜²æ­¢studentçš„è¡¨ç°è¢«teacheré™åˆ¶ï¼Œåœ¨è’¸é¦æ—¶é€æ¸å‡å°‘soft targetsçš„æƒé‡\n",
    "\n",
    "### åŸºæœ¬æ€æƒ³\n",
    "1.1 ä¸ºä»€ä¹ˆè’¸é¦å¯ä»¥work\n",
    "\n",
    "å¥½æ¨¡å‹çš„ç›®æ ‡ä¸æ˜¯æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯å­¦ä¹ å¦‚ä½•æ³›åŒ–åˆ°æ–°çš„æ•°æ®ã€‚æ‰€ä»¥è’¸é¦çš„ç›®æ ‡æ˜¯è®©studentå­¦ä¹ åˆ°teacherçš„æ³›åŒ–èƒ½åŠ›ï¼Œç†è®ºä¸Šå¾—åˆ°çš„ç»“æœä¼šæ¯”å•çº¯æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„studentè¦å¥½ã€‚å¦å¤–ï¼Œå¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œå¦‚æœsoft targetsçš„ç†µæ¯”hard targetsé«˜ï¼Œé‚£æ˜¾ç„¶studentä¼šå­¦ä¹ åˆ°æ›´å¤šçš„ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# domain-adaptive  pretraining\n",
    "https://mp.weixin.qq.com/s/qULq9ye_Pg56pEQIdvr8tQ\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/149210123\n",
    "\n",
    "ACL2020 Best Paperæœ‰ä¸€ç¯‡è®ºæ–‡æåå¥–ï¼Œã€ŠDonâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasksã€‹ã€‚è¿™ç¯‡è®ºæ–‡åšäº†å¾ˆå¤šè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å®éªŒï¼Œç³»ç»Ÿçš„åˆ†æäº†è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯¹å­ä»»åŠ¡çš„æ•ˆæœæå‡æƒ…å†µã€‚æœ‰å‡ ä¸ªä¸»è¦ç»“è®ºï¼š\n",
    "\n",
    "åœ¨ç›®æ ‡é¢†åŸŸçš„æ•°æ®é›†ä¸Šç»§ç»­é¢„è®­ç»ƒï¼ˆDAPTï¼‰å¯ä»¥æå‡æ•ˆæœï¼›ç›®æ ‡é¢†åŸŸçš„è¯­æ–™ä¸RoBERTaçš„åŸå§‹é¢„è®­ç»ƒè¯­æ–™è¶Šä¸ç›¸å…³ï¼ŒDAPTæ•ˆæœåˆ™æå‡æ›´æ˜æ˜¾ã€‚\n",
    "\n",
    "åœ¨å…·ä½“ä»»åŠ¡çš„æ•°æ®é›†ä¸Šç»§ç»­é¢„è®­ç»ƒï¼ˆTAPTï¼‰å¯ä»¥ååˆ†â€œå»‰ä»·â€åœ°æå‡æ•ˆæœã€‚\n",
    "\n",
    "ç»“åˆäºŒè€…ï¼ˆå…ˆè¿›è¡ŒDAPTï¼Œå†è¿›è¡ŒTAPTï¼‰å¯ä»¥è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚\n",
    "\n",
    "å¦‚æœèƒ½è·å–æ›´å¤šçš„ã€ä»»åŠ¡ç›¸å…³çš„æ— æ ‡æ³¨æ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼ˆCurated-TAPTï¼‰ï¼Œæ•ˆæœåˆ™æœ€ä½³ã€‚\n",
    "\n",
    "å¦‚æœæ— æ³•è·å–æ›´å¤šçš„ã€ä»»åŠ¡ç›¸å…³çš„æ— æ ‡æ³¨æ•°æ®ï¼Œé‡‡å–ä¸€ç§ååˆ†è½»é‡åŒ–çš„ç®€å•æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œæ•ˆæœä¹Ÿä¼šæå‡ã€‚\n",
    "\n",
    "ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ç¯‡paperï¼Œæˆ‘ä»¬éœ€è¦ç‰¢è®°ä¸¤ä¸ªé‡è¦çš„ä¸“æœ‰åè¯ï¼š\n",
    "\n",
    "* DAPTï¼šé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒ(Domain-Adaptive Pretraining)ï¼Œå°±æ˜¯åœ¨æ‰€å±çš„é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒï½\n",
    "* TAPTï¼šä»»åŠ¡è‡ªé€‚åº”é¢„è®­ç»ƒ(Task-Adaptive Pretraining)ï¼Œå°±æ˜¯åœ¨å…·ä½“ä»»åŠ¡æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒï½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/2004.10964\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1071b2f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/2004.10964', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Language  models  pretrained  on  text  from  a wide  variety  of  sources  form  the  foundation of  todayâ€™s  NLP.  In  light  of  the  success  of these  broad-coverage  models,  we  investigate whether it is still helpful to tail or a pretrained model  to  the  domain  of  a  target  task.    \n",
    "\n",
    "We present a study across four domains (\n",
    "* biomedical and \n",
    "* computer science publications,  \n",
    "* news,and  \n",
    "* reviews)  and  \n",
    "\n",
    "eight  classification  tasks,showing that a second phase of pretraining in-domain  (domain-adaptive  pretraining)  leads to  performance  gains,  under  both  high- and low-resource  settings.\n",
    "\n",
    "Moreover,   adapting to  the  taskâ€™s  unlabeled  data  (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. \n",
    "\n",
    "Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable.  Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch transformers é¢„è®­ç»ƒæ¨¡å‹\n",
    "https://mp.weixin.qq.com/s/qULq9ye_Pg56pEQIdvr8tQ\n",
    "\n",
    "è™½ç„¶åœ¨bertä¸Šè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåœ¨ç®—æ³•æ¯”èµ›ä¸­å·²ç»æ˜¯ä¸€ä¸ªç¨³å®šçš„ä¸Šåˆ†æ“ä½œã€‚ä½†æ˜¯ä¸Šé¢è¿™ç¯‡æ–‡ç« éš¾èƒ½å¯è´µçš„æ˜¯å¯¹è¿™ä¸ªæ“ä½œè¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚å¤§éƒ¨åˆ†ä¸­æ–‡è¯­è¨€æ¨¡å‹éƒ½æ˜¯åœ¨tensorflowä¸Šè®­ç»ƒçš„ï¼Œä¸€ä¸ªå¸¸è§ä¾‹å­æ˜¯ä¸­æ–‡robertaé¡¹ç›®ã€‚å¯ä»¥å‚è€ƒ\n",
    "\n",
    "https://github.com/brightmart/roberta_zh\n",
    "\n",
    "\n",
    "\n",
    "ä½¿ç”¨pytorchè¿›è¡Œä¸­æ–‡bertè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„ä¾‹å­æ¯”è¾ƒå°‘ã€‚åœ¨huggingfaceçš„Transformersä¸­ï¼Œæœ‰ä¸€éƒ¨åˆ†ä»£ç æ”¯æŒè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ(ä¸æ˜¯å¾ˆä¸°å¯Œï¼Œå¾ˆå¤šåŠŸèƒ½éƒ½ä¸æ”¯æŒæ¯”å¦‚wwm)ã€‚ä¸ºäº†ç”¨æœ€å°‘çš„ä»£ç æˆæœ¬å®Œæˆbertè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼Œæœ¬æ–‡å€Ÿé‰´äº†é‡Œé¢çš„ä¸€äº›ç°æˆä»£ç ã€‚ä¹Ÿå°è¯•åˆ†äº«ä¸€ä¸‹ä½¿ç”¨pytorchè¿›è¡Œè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„ä¸€äº›ç»éªŒã€‚ä¸»è¦æœ‰ä¸‰ä¸ªå¸¸è§çš„ä¸­æ–‡bertè¯­è¨€æ¨¡å‹\n",
    "\n",
    "bert-base-chinese\n",
    "\n",
    "roberta-wwm-ext\n",
    "\n",
    "ernie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-chinese\n",
    "\n",
    "\n",
    "\n",
    "(https://huggingface.co/bert-base-chinese)\n",
    "\n",
    "è¿™æ˜¯æœ€å¸¸è§çš„ä¸­æ–‡bertè¯­è¨€æ¨¡å‹ï¼ŒåŸºäºä¸­æ–‡ç»´åŸºç™¾ç§‘ç›¸å…³è¯­æ–™è¿›è¡Œé¢„è®­ç»ƒã€‚æŠŠå®ƒä½œä¸ºbaselineï¼Œåœ¨é¢†åŸŸå†…æ— ç›‘ç£æ•°æ®è¿›è¡Œè¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¾ˆç®€å•ã€‚åªéœ€è¦ä½¿ç”¨å®˜æ–¹ç»™çš„ä¾‹å­å°±å¥½ã€‚\n",
    "\n",
    "https://github.com/huggingface/transformers/tree/master/examples/language-modeling\n",
    "\n",
    "(æœ¬æ–‡ä½¿ç”¨çš„transformersæ›´æ–°åˆ°3.0.2)\n",
    "\n",
    "æ–¹æ³•å°±æ˜¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=bert \\\n",
    "    --model_name_or_path=bert-base-chinese \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$TEST_FILE \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶ä¸­$TRAIN_FILE ä»£è¡¨é¢†åŸŸç›¸å…³ä¸­æ–‡è¯­æ–™åœ°å€ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roberta-wwm-ext\n",
    "\n",
    "(https://github.com/ymcui/Chinese-BERT-wwm)\n",
    "\n",
    "å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚é¢„è®­ç»ƒçš„æ–¹å¼æ˜¯é‡‡ç”¨robertaç±»ä¼¼çš„æ–¹æ³•ï¼Œæ¯”å¦‚åŠ¨æ€maskï¼Œæ›´å¤šçš„è®­ç»ƒæ•°æ®ç­‰ç­‰ã€‚åœ¨å¾ˆå¤šä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹æ•ˆæœè¦ä¼˜äºbert-base-chineseã€‚\n",
    "\n",
    "å¯¹äºä¸­æ–‡robertaç±»çš„pytorchæ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "roberta = BertModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ‡è®°ä¸å¯ä½¿ç”¨å®˜æ–¹æ¨èçš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'chinese-roberta-wwm-ext'  # é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜ç›®å½• \n",
    "model.save_pretrained(dir_)  # ä¼šç”Ÿæˆ æ¨¡å‹binæ–‡ä»¶å’Œconfig.json\n",
    "tokenizer.save_pretrained(dir_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºä¸­æ–‡robertaç±»çš„é…ç½®æ–‡ä»¶æ¯”å¦‚vocab.txtï¼Œéƒ½æ˜¯é‡‡ç”¨bertçš„æ–¹æ³•è®¾è®¡çš„ã€‚è‹±æ–‡robertaæ¨¡å‹è¯»å–é…ç½®æ–‡ä»¶çš„æ ¼å¼é»˜è®¤æ˜¯vocab.jsonã€‚å¯¹äºä¸€äº›è‹±æ–‡robertaæ¨¡å‹ï¼Œå€’æ˜¯å¯ä»¥é€šè¿‡AutoModelè‡ªåŠ¨è¯»å–ã€‚è¿™å°±è§£é‡Šäº†huggingfaceçš„æ¨¡å‹åº“çš„ä¸­æ–‡robertaç¤ºä¾‹ä»£ç ä¸ºä»€ä¹ˆè·‘ä¸é€šã€‚https://huggingface.co/models?\n",
    "\n",
    "å¦‚æœè¦åŸºäºä¸Šé¢çš„ä»£ç run_language_modeling.pyç»§ç»­é¢„è®­ç»ƒrobertaã€‚è¿˜éœ€è¦åšä¸¤ä¸ªæ”¹åŠ¨ã€‚\n",
    "\n",
    "ä¸‹è½½roberta-wwm-extåˆ°æœ¬åœ°ç›®å½•hflrobertaï¼Œåœ¨config.jsonä¸­ä¿®æ”¹â€œmodel_typeâ€:\"roberta\"ä¸º\"model_type\":\"bert\"ã€‚\n",
    "\n",
    "å¯¹ä¸Šé¢çš„run_language_modeling.pyä¸­çš„AutoModelå’ŒAutoTokenizeréƒ½è¿›è¡Œæ›¿æ¢ä¸ºBertModelå’ŒBertTokenizerã€‚\n",
    "\n",
    "å†è¿è¡Œå‘½ä»¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_language_modeling_roberta.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=bert \\\n",
    "    --model_name_or_path=hflroberta \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$TEST_FILE \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ernie\n",
    "\n",
    "ï¼ˆhttps://github.com/nghuyong/ERNIE-Pytorch ï¼‰\n",
    "\n",
    "ernieæ˜¯ç™¾åº¦å‘å¸ƒçš„åŸºäºç™¾åº¦çŸ¥é“è´´å§ç­‰ä¸­æ–‡è¯­æ–™ç»“åˆå®ä½“é¢„æµ‹ç­‰ä»»åŠ¡ç”Ÿæˆçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹çš„å‡†ç¡®ç‡åœ¨æŸäº›ä»»åŠ¡ä¸Šè¦ä¼˜äºbert-base-chineseå’Œrobertaã€‚å¦‚æœåŸºäºernie1.0æ¨¡å‹åšé¢†åŸŸæ•°æ®é¢„è®­ç»ƒçš„è¯åªéœ€è¦ä¸€æ­¥ä¿®æ”¹ã€‚\n",
    "\n",
    "ä¸‹è½½ernie1.0åˆ°æœ¬åœ°ç›®å½•ernieï¼Œåœ¨config.jsonä¸­å¢åŠ å­—æ®µ\"model_type\":\"bert\"ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=bert \\\n",
    "    --model_name_or_path=ernie \\\n",
    "    --do_train \\\n",
    "    --train_data_file=$TRAIN_FILE \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=$TEST_FILE \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼Œhuggingfaceé¡¹ç›®ä¸­è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç”¨maskæ–¹å¼å¦‚ä¸‹ã€‚ä»æ˜¯æŒ‰ç…§15%çš„æ•°æ®éšæœºmaskç„¶åé¢„æµ‹è‡ªèº«ã€‚å¦‚æœè¦åšä¸€äº›é«˜çº§æ“ä½œæ¯”å¦‚whole word maskingæˆ–è€…å®ä½“é¢„æµ‹ï¼Œå¯ä»¥è‡ªè¡Œä¿®æ”¹transformers.DataCollatorForLanguageModelingã€‚\n",
    "\n",
    "æœ¬æ–‡å®éªŒä»£ç åº“ã€‚æ‹¿æ¥å³ç”¨ï¼\n",
    "\n",
    "https://github.com/zhusleep/pytorch_chinese_lm_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
