{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 李宏毅-深度学习\n",
    "https://www.bilibili.com/video/BV1b4411S7Ky/?spm_id_from=333.788.videocard.4\n",
    "\n",
    "定义一个神经网络的三个步骤:\n",
    "1. build nn\n",
    "2. 确定cost function\n",
    "3. 确定优化函数\n",
    "![](deep_learning/img/dl01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review basic structure\n",
    "https://www.bilibili.com/video/BV1b4411S7Ky/?spm_id_from=333.788.videocard.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fully connected layer\n",
    "Dense层在数学上看，就是 输入n为的向量Vn乘以一个矩阵，transform为m维的向量Vm，再经过激活函数做非线性变换\n",
    "\n",
    "$$V_m=\\sigma(V_n*W_{n*m})$$\n",
    "### 符号表示\n",
    "![](deep_learning/img/dl02.png)\n",
    "![](deep_learning/img/dl03.png)\n",
    "![](deep_learning/img/dl04.png)\n",
    "![](deep_learning/img/dl05.png)\n",
    "![](deep_learning/img/dl06.png)\n",
    "![](deep_learning/img/dl07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Structure\n",
    "![](deep_learning/img/dr01.png)\n",
    "### Deep RNN\n",
    "![](deep_learning/img/dr02.png)\n",
    "\n",
    "### bidirectional RNN \n",
    "![](deep_learning/img/dr04.png)\n",
    "\n",
    "### pyramidal RNN\n",
    "DeepRNN, 合并时间步长\n",
    "![](deep_learning/img/dr05.png)\n",
    "### Naive RNN\n",
    "![](deep_learning/img/dn01.png)\n",
    "### LSTM\n",
    "![](deep_learning/img/dn03.png)\n",
    "![](deep_learning/img/dn04.png)\n",
    "![](deep_learning/img/dn06.png)\n",
    "$\\odot$表示element-wise的相乘 \n",
    "\n",
    "向量Zi: input gate,决定输入能否流进去\n",
    "\n",
    "向量Zf: forget gate, 决ct-1能否传入下一个时间点\n",
    "\n",
    "向量Zo: output gate,\n",
    "![](deep_learning/img/dn07.png)\n",
    "![](deep_learning/img/dn08.png)\n",
    "\n",
    "\n",
    "![](deep_learning/img/lstmresult.png)\n",
    "### GRU\n",
    "![](deep_learning/img/dn09.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN | Pooling\n",
    "https://www.bilibili.com/video/BV1b4411S7Ky/?p=2  开始时间 5min\n",
    "\n",
    "Simplify nn(based on prior knowledge of the task\n",
    "\n",
    "![](deep_learning/img/cnn01.png)\n",
    "![](deep_learning/img/cnn02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conception\n",
    "* receptive field  \n",
    "    感受野，来自生物学概念\n",
    "* filter(kernel)\n",
    "    * filter(kernel) size  \n",
    "        该filter覆盖的感受野的大小\n",
    "    * Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积对应什么样的数学操作\n",
    "CNN的卷积运算为何使用互相关而不是卷积 https://blog.csdn.net/appleyuchi/article/details/86574350\n",
    "\n",
    "通俗理解【卷】积+互相关与卷积 https://blog.csdn.net/Sunny_HQ/article/details/80875664\n",
    "\n",
    "\n",
    "一个二维矩阵经过卷积核处理后是什么结果？当kernel划过对应的感受野时，对应的数学运算叫互相关(即把kernel对应位置和其覆盖的位置对应相乘，然后结果求和)\n",
    "![](deep_learning/img/cnnm01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN的卷积运算并非数学定义的卷积\n",
    "\n",
    "也就是说，CNN中的运算是不需要翻转卷积核的。\n",
    "\n",
    "也就是说，CNN在处理图像时的卷积核是不需要翻转180°的\n",
    "\n",
    "我们来用代码看下为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------卷积结果---------------------\n",
      "[[0.00e+00 3.00e+00 3.00e-01]\n",
      " [6.00e+00 3.06e+01 3.00e+02]\n",
      " [6.00e+01 6.00e+02 0.00e+00]]\n",
      "---------互相关结果---------------------\n",
      "[[  0.  300.   30. ]\n",
      " [600.   60.3   3. ]\n",
      " [  0.6   6.    0. ]]\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "A=np.array([[0,0,0,0],\\\n",
    "                [0,0,30,0],\\\n",
    "                [0,60,0,0],\\\n",
    "                [0,0,0,0]])\n",
    "\n",
    "B=np.array([[0.1,0.01],\\\n",
    "                [1,10]])\n",
    "import scipy.signal\n",
    "print(\"---------卷积结果---------------------\")\n",
    "print( scipy.signal.convolve(A,B,mode='valid'))\n",
    "print(\"---------互相关结果---------------------\")\n",
    "print( scipy.signal.correlate(A,B,mode='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为卷积层后面连接的是池化层，\n",
    "也就是说把卷积结果得到的矩阵中，\n",
    "选取矩阵中数值最大的元素作为保留，矩阵中其余元素一律删除。\n",
    "\n",
    "所以我们可以看到：\n",
    "上述代码的\n",
    "卷积结果中的最大值\n",
    "与\n",
    "互相关结果矩阵中的最大值\n",
    "都是600\n",
    "因此后面maxpooling层进行池化后得到的值也都是600.\n",
    "\n",
    "如果后面接全连接层，那么\n",
    "上面两个矩阵全部flatten以后输入dense层，几乎完全一致，也不影响建模\n",
    "\n",
    "结论：\n",
    "CNN中使用卷积或互相关，对于\n",
    "的贡献是一致的，都是获取像素最大的那个值，\n",
    "因此可以使用卷积，也可以使用互相关，\n",
    "但是为了代码的高效，直接使用“互相关”即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example-1D signal+Multiple Channel\n",
    "![](deep_learning/img/cnn03.png)\n",
    "![](deep_learning/img/cnn04.png)\n",
    "![](deep_learning/img/cnn05.png)\n",
    "![](deep_learning/img/cnn06.png)\n",
    "![](deep_learning/img/cnn07.png)\n",
    "![](deep_learning/img/cnnp01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph\n",
    "https://www.bilibili.com/video/BV1b4411S7Ky/?p=3\n",
    "![](deep_learning/img/cg01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Network 前馈网络的计算图画法\n",
    "$$y=\\sigma(W^L...\\sigma(W^2\\sigma(W^1x+b^1)+b^2)...+b^L)$$\n",
    "![](deep_learning/img/cgl01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function of Feedforward Network\n",
    "![](deep_learning/img/cgl02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of Cost Function\n",
    "![](deep_learning/img/cg02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向求微分时，遇到的问题是: C是标量，而y, z2, a1, z1等都是向量，标量对向量，向量对向量如何求偏微分呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian Matrix\n",
    "向量对向量求偏微分的结果，是一个矩阵，行数是分子向量的维度，列数是分母向量的维度\n",
    "![](deep_learning/img/cg03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例求解\n",
    "#### $\\frac{\\partial C}{\\partial{y}}$\n",
    "以loss function为cross entropy loss为例说明\n",
    "$$CrossEntropyLoss: l(\\vec y, \\vec{\\hat y})=-\\sum_{i=1}^n\\hat{y_i}ln(y_i)=-ln(y_r)$$，其中$\\hat{y_i}$的第r维为1，其余为0\n",
    "其中$\\vec y$为预测值，$\\vec{\\hat{y}}$为真实值, \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial{y}}=[0 ... \\frac{-1}{y_r} ...]$$\n",
    "![](deep_learning/img/cgr01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial y}{\\partial z^2}$\n",
    "以\\sigma为sigmoid函数为例\n",
    "$$\\vec y=[y_1\\ y_2\\ ...]=[\\sigma(z_1^2)\\ (\\sigma z_2^2)\\ ....]$$\n",
    "![](deep_learning/img/cgr02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial z^2}{\\partial a^1}$\n",
    "$$\\frac{\\partial z^2}{\\partial a^1}=W^2$$\n",
    "![](deep_learning/img/cgr03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{\\partial z^2}{\\partial W^2}$\n",
    "把矩阵拉平为向量\n",
    "![](deep_learning/img/cgr04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\frac{C}{\\partial W^1}$\n",
    "![](deep_learning/img/cgr05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph for Recurrent Network\n",
    "输入为时间步长为1步\n",
    "![](deep_learning/img/cgr11.png)\n",
    "输入为时间步长为3步\n",
    "![](deep_learning/img/cgr12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](deep_learning/img/cgr14.png)\n",
    "![](deep_learning/img/cgr15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Transformer\n",
    "https://www.bilibili.com/video/BV1b4411S7Ky?p=8  25min attention  \n",
    "\n",
    "传统的seq2seq模型，先由encoder把一个句子编码为一个向量，然后把这个向量作为decoder的输入，每次时间步都输入，直到decoder产生结束标志为止。\n",
    "\n",
    "传统的seq2seq模型的问题是 整个句子被编码为一个向量，decoder看到的是相同的。\n",
    "![](deep_learning/img/att00.png)\n",
    "而以机器翻译为例，输入句子的不同部分对输出的影响是不同的。所以引出了attention\n",
    "![](deep_learning/img/att01.png)\n",
    "z0是一个初始的向量，其参数可以学出来\n",
    "![](deep_learning/img/att02.png)\n",
    "z1是把c0丢进decoder中rnn的隐层输出，\n",
    "![](deep_learning/img/att03.png)\n",
    "把z1再和h1...等算一次得到c1\n",
    "![](deep_learning/img/att04.png)\n",
    "![](deep_learning/img/att06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 花书\n",
    "http://www.deeplearningbook.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
