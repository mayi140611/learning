{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax approximations\n",
    "https://zhuanlan.zhihu.com/p/129824834\n",
    "    \n",
    "https://ruder.io/word-embeddings-softmax/\n",
    "\n",
    "https://discuss.pytorch.org/t/sampled-softmax-loss/305/11  PYTHON 实现sampled-softmax-loss\n",
    "\n",
    "The softmax layer is a core part of many current neural network architectures. When the number of output classes is very large, such as in the case of language modelling, computing the softmax becomes very expensive. This post explores approximations to make the computation more efficient.\n",
    "\n",
    "softmax分母计算速度慢的问题，ruder的博客认为解决有两种方式，一种是Softmax-based Approaches，保持softmax层不变，但修改其架构以提高其效率，比如分层softmax；一种是Sampling-based Approaches，比如通过采样优化损失函数，来接近softmax。\n",
    "\n",
    "sampled softmax，就是第二种的一种，通过采样来近似softmax的分母，（NCE，负采样是第二种的另一种，可看ruder的博客，主要是通过舍弃softmax，构建其他形式损失函数来近似）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax-based Approaches\n",
    "## Hierarchical Softmax\n",
    "Hierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio (2005) [3]. H-Softmax essentially __replaces the flat softmax layer with a hierarchical layer__ that has the words as leaves, as can be seen in Figure 1.\n",
    "## Differentiated Softmax\n",
    "## CNN-Softmax\n",
    "# Sampling-based Approaches\n",
    "        Importance Sampling\n",
    "        Adaptive Importance Sampling\n",
    "        Target Sampling\n",
    "## Noise Contrastive Estimation( NCE )\n",
    "## Negative Sampling(NEG)\n",
    "Negative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples k\n",
    "\n",
    "increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.\n",
    "\n",
    "NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word w\n",
    "comes from the empirical training distribution Ptrain given a context c as follows:\n",
    "\n",
    "        Self-Normalisation\n",
    "        Infrequent Normalisation\n",
    "        Other Approaches\n",
    "# Which Approach to Choose?\n",
    "\n",
    "\n",
    "    Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
