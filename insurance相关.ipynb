{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Claim: Payer Response Prediction from Claims Data with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/2007.06229.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10fc88940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/2007.06229.pdf', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Each year, almost 10% of claims are denied by payers (i.e., health insurance plans). With the cost to recover these denials and underpayments, predicting payer response (likelihood of payment) from claims data with a high degree of accuracyand precision is anticipated to improve healthcare staffs’ performance productivity and drive better patient financial experience and satisfaction in the revenue cycle (Barkholz, 2017).  \n",
    "\n",
    "However, con-structing advanced predictive analytics models has been considered challenging in the last twentyyears. That said, \n",
    "\n",
    "we propose __a (low-level) context-dependent compact representation of patients’ historical claim records by effectively learning complicated dependencies in the (high-level) claim inputs__.  \n",
    "\n",
    "Built on this new latent representation,we demonstrate that __a deep learning-based frame-work__, Deep Claim, can accurately predict various responses from multiple payers using 2,905,026 de-identified claims data from two US health sys-tems. Deep Claim’s improvements over carefully chosen baselines in predicting claim denials aremost pronounced as 22.21% relative recall gain(at 95% precision) on Health System A, whichimplies Deep Claim can find 22.21% more denials than the best baseline system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Claim\n",
    "We propose Deep Claim as a neural network-based system to predict whether, when, and how much a payer will pay for each claim. \n",
    "\n",
    "Deep Claim takes the claims data composed of \n",
    "* demographic(人口统计数据) information, \n",
    "* diagnoses, \n",
    "* treatments, and \n",
    "* billed amounts(账单金额 \n",
    "\n",
    "as an input. \n",
    "\n",
    "Given that, Deep Claim predicts \n",
    "* the first response date, \n",
    "* denial probability, \n",
    "* denial reason codes with probability, and \n",
    "* questionable fields in the claim. \n",
    "\n",
    "In this section, we describe the Deep Claim model in detail, which the complete architecture illustrated in Figure 1.\n",
    "\n",
    "Figure 1.Architecture of a Deep Claim for the payer response prediction as described in Section 3.\n",
    "\n",
    "### 3.1. Claims Input Representation \n",
    "The claim vector we create from the raw claim is composed of a huge number of variables (i.e., features) - \n",
    "* subscriber gender, \n",
    "* an individual relationship code, \n",
    "* a payer state, \n",
    "* the duration of the corresponding service, \n",
    "* the subscriber’s age,\n",
    "* the patient’s age, \n",
    "* a payer identifier, \n",
    "* the total charges, \n",
    "* the services date, and \n",
    "* transmission of the claim date. \n",
    "\n",
    "The claim vector also includes an indication of procedures performed and diagnoses received.  The value of each feature is as-signed a single unique token for singular elements or sub-context vectors of tokens for procedures and diagnoses (that can have multiple values).We tokenize procedures and diagnoses and map them toa sub-context vector of tokens.  Less frequent tokens are mapped to an out-of-vocabulary (OOV) token (for example,procedure token appears less than 500 times in the dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also normalize numeric values. \n",
    "* The date is mapped totokens in years, months, and days.  \n",
    "* The charge amount in dollars is quantized to thousands, hundreds, tens, and ones. \n",
    "* The patient’s age is discretized in years.\n",
    "\n",
    "After defining the features, we categorize them into three contextual categories: \n",
    "* procedure, \n",
    "* diagnosis, and \n",
    "* other features regarding the claim, such as demographic patient information.   \n",
    "\n",
    "Procedures  and  diagnosis  token  vectors  can be expressed as a normalized count vector (e.g., relative frequency) xc and xd with a length of the possible procedure and diagnosis tokens respectively. All the other single unique feature tokens can be comprised as xo, which is a binary vector of a length of the total number of single unique tokens. One can piece all of them together to converta single claim to a concatenated vector x as (xc,xd,xo). Typically, this vector x can have a length in the thousands and be the extremely sparse vector.\n",
    "\n",
    "### 3.2. Claims Embedding Network\n",
    "Unlike natural language sentences,  the extremely sparse vectorx is an unordered collection of medical events and aggregations of diverse code types that encapsulates various aspects of complicate dependencies.  So it is not straight-forward to apply off-the-shelf NLP embedding techniquesfor compressing this sparse vector into a fixed-sized latentvectorh(94  in  our  experiments).   Instead,  we  leveragegating mechanism, which is essential for recurrent neuralnetworks  (Hochreiter  &  Schmidhuber,  1997;  Cho  et  al.,2014) and bilinear models (Tenenbaum & Freeman, 2000;Kim et al., 2017) that provide richer representations thanlinear models.  To be specific, we propose the followingnovel methods to learn effective embedding representationmappingsH:x→hof each claim by activating the gateover each context sub-vector to extract inter-componentdependencies within each category and combining themfurther to learn intra-dependencies among the context sub-vectors by taking the pairwise inner product in the latentlow-dimensional space.First,   we  convert  each  sub-category  vector  to  lower-dimensional context vectorsf(0,i)simply asσ(W(0,i)fxi+b(0,i)f)whereWfis the low-dimensional embedding ma-trix andσis a ReLU (Nair & Hinton, 2010) function fori={c,d,o}. Then, the context vectorsc(0,i)modulated bythe gates is represented asf(0,i)g(0,i)(W(0,i)gxi+b(0,i)g)wheregis the Softmax function anddenotes element-wise multiplication. These gate activation values over eachsub-vector can be viewed as dynamic importance scores ofthe (high-level) input feature that enables learnable featureselection and simultaneous dimensionality reduction whilehandling sparsity in each sub-vector.  To further increasethe hierarchy of gated layers like a probabilistic decisiontree, we add one more set of gated networks forc(1,c)in the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
