{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla(普通的) Transformer\n",
    "https://zhuanlan.zhihu.com/p/84159401\n",
    "\n",
    "transformer作为一种特征提取器，在NLP中有广泛的应用。但是Trm需要对输入序列设置一个固定的长度，比如在BERT中，默认长度是512。如果文本序列长度短于固定长度，可以通过填充的方式来解决。如果序列长度超过固定长度，处理起来就比较麻烦。一种处理方式，就是将文本划分为多个segments。训练的时候，对每个segment单独处理，segments之间没有联系，如下图(a)所示。这存在两个问题，1）因为segments之间独立训练，所以不同的token之间，最长的依赖关系，就取决于segment的长度；2）出于效率的考虑，在划分segments的时候，不考虑句子的自然边界，而是根据固定的长度来划分序列，导致分割出来的segments在语义上是不完整的。\n",
    "\n",
    "在预测的时候，会对固定长度的segment做计算，一般取最后一个位置的隐向量作为输出。为了充分利用上下文关系，在每做完一次预测之后，就对整个序列向右移动一个位置，再做一次计算，如上图(b)所示，这导致计算效率非常低。\n",
    "![](img/txl01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章Transformer-XL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1808.04444\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110dc8ba8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1808.04444', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章Transformer-XL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1901.02860\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110dc8400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1901.02860', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Transformers   have   a   potential   of   learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We  propose  a  novel  neural  ar-chitecture Transformer-XL that enables learn-ing  dependency  beyond  a  fixed  length  with-out  disrupting  temporal  coherence.    \n",
    "\n",
    "It  con-sists of a segment-level recurrence mechanism and a novel positional encoding scheme.  Our method not only enables capturing longer-term dependency, but also resolves the context frag-mentation problem.  As a result, Transformer-XL learns dependency that is 80% longer than RNNs  and  450%  longer  than  vanilla  Trans-formers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.  Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on en-wiki8,  1.08  on  text8,  18.3  on  WikiText-103,21.8 on One Billion Word, and 54.5 on PennTreebank (without finetuning).  When trained only on WikiText-103, Transformer-XL man-ages  to  generate  reasonably  coherent,  noveltext  articles  with  thousands  of  tokens.    Ourcode, pretrained models, and hyperparametersare available in both Tensorflow and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"\n",
    "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price.\n",
    "The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal.\n",
    "Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008.\n",
    "Real estate firm Tishman Speyer had owned the other 10%.\n",
    "The buyer is RFR Holding, a New York real estate company.\n",
    "Officials with Tishman and RFR did not immediately respond to a request for comments.\n",
    "It's unclear when the deal will close.\n",
    "The building sold fairly quickly after being publicly placed on the market only two months ago.\n",
    "The sale was handled by CBRE Group.\n",
    "The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building.\n",
    "The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028.\n",
    "Meantime, rents in the building itself are not rising nearly that fast.\n",
    "While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor-to-ceiling windows and all the modern amenities.\n",
    "Still the building is among the best known in the city, even to people who have never been to New York.\n",
    "It is famous for its triangle-shaped, vaulted windows worked into the stylized crown, along with its distinctive eagle gargoyles near the top.\n",
    "It has been featured prominently in many films, including Men in Black 3, Spider-Man, Armageddon, Two Weeks Notice and Independence Day.\n",
    "The previous sale took place just before the 2008 financial meltdown led to a plunge in real estate prices.\n",
    "Still there have been a number of high profile skyscrapers purchased for top dollar in recent years, including the Waldorf Astoria hotel, which Chinese firm Anbang Insurance purchased in 2016 for nearly $2 billion, and the Willis Tower in Chicago, which was formerly known as Sears Tower, once the world's tallest.\n",
    "Blackstone Group (BX) bought it for $1.3 billion 2015.\n",
    "The Chrysler Building was the headquarters of the American automaker until 1953, but it was named for and owned by Chrysler chief Walter Chrysler, not the company itself.\n",
    "Walter Chrysler had set out to build the tallest building in the world, a competition at that time with another Manhattan skyscraper under construction at 40 Wall Street at the south end of Manhattan. He kept secret the plans for the spire that would grace the top of the building, building it inside the structure and out of view of the public until 40 Wall Street was complete.\n",
    "Once the competitor could rise no higher, the spire of the Chrysler building was raised into view, giving it the title.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2755"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
