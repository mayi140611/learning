{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers: Multilingual Sentence Embeddings using BERT / RoBERTa / XLM-RoBERTa & Co. with PyTorch\n",
    "https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "\n",
    "BERT / RoBERTa / XLM-RoBERTa produces out-of-the-box rather bad sentence embeddings. \n",
    "\n",
    "This repository fine-tunes BERT / RoBERTa / DistilBERT / ALBERT / XLNet with __a siamese or triplet network structure__ to produce semantically meaningful sentence embeddings that can be used in unsupervised scenarios: Semantic textual similarity via cosine-similarity, clustering, semantic search.\n",
    "\n",
    "We provide an increasing number of state-of-the-art pretrained models that can be used to derive sentence embeddings. See Pretrained Models. Details of the implemented approaches can be found in our publication: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (EMNLP 2019).\n",
    "\n",
    "You can use this code to easily train your own sentence embeddings, that are tuned for your specific task. We provide various dataset readers and you can tune sentence embeddings with different loss function, depending on the structure of your dataset. For further details, see Train your own Sentence Embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/71/acfb3f1016f83d90590130dc2ee0d8cd36b005aa7afa45b465837b711070/sentence-transformers-0.3.3.tar.gz (65kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 336kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: transformers>=3.0.2 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (4.35.0)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.19.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (2020.7.14)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.1.91)\n",
      "Collecting tokenizers==0.8.1.rc1 (from transformers>=3.0.2->sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/c8/b07f4346b36ca83988a4a59c081156ec2c96aad5b4c448c75deea4f53356/tokenizers-0.8.1rc1-cp37-cp37m-macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: packaging in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (19.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from transformers>=3.0.2->sentence-transformers) (3.0.10)\n",
      "Requirement already satisfied, skipping upgrade: future in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from torch>=1.2.0->sentence-transformers) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (3.4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (2018.4.16)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from requests->transformers>=3.0.2->sentence-transformers) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /Users/luoyonggui/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers>=3.0.2->sentence-transformers) (6.7)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/luoyonggui/Library/Caches/pip/wheels/75/d6/0a/cab163b21d0597cc1580bc344487b11ad405e0d1d314725f2b\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers, tokenizers\n",
      "  Found existing installation: tokenizers 0.8.1\n",
      "    Uninstalling tokenizers-0.8.1:\n",
      "      Successfully uninstalled tokenizers-0.8.1\n",
      "Successfully installed sentence-transformers-0.3.3 tokenizers-0.8.1rc1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers==0.8.1\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/3e/7cf9b5daa88371c96d9b63d31917e30ba93b1d89421aef79c00e806bc54d/tokenizers-0.8.1-cp37-cp37m-macosx_10_11_x86_64.whl\n",
      "\u001b[31mERROR: transformers 3.0.2 has requirement tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tokenizers\n",
      "  Found existing installation: tokenizers 0.8.1rc1\n",
      "    Uninstalling tokenizers-0.8.1rc1:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc1\n",
      "Successfully installed tokenizers-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/1908.10084\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110e501d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/1908.10084', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "BERT (Devlin et al., 2018) and RoBERTa (Liuet  al.,  2019)  has  set  a  new  state-of-the-art performance on sentence-pair regression tasks like  semantic  textual  similarity  (STS).  How-ever,  it  requires  that  both  sentences  are  fed into the network, which causes a massive com-putational  overhead:   Finding  the  most  sim-ilar  pair  in  a  collection  of  10,000  sentences requires about 50 million inference computa-tions (~65 hours) with BERT. \n",
    "\n",
    "The construction of BERT makes it unsuitable for semantic sim-ilarity search as well as for unsupervised tasks like clustering.\n",
    "\n",
    "In this publication, we present Sentence-BERT(SBERT),  a  modification  of  the  pretrained BERT network that use siamese and triplet net-work structures to derive semantically mean-ingful sentence embeddings that can be com-pared using cosine-similarity. This reduces the effort for finding the most similar pair from 65hours with BERT / RoBERTa to about 5 sec-onds with SBERT, while maintaining the ac-curacy from BERT. \n",
    "\n",
    "We evaluate SBERT and SRoBERTa on com-mon  STS  tasks  and  transfer  learning  tasks,where   it   outperforms   other   state-of-the-art sentence embeddings methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started-Sentences Embedding with a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0821 14:41:04.881035 140736034558848 file_utils.py:39] PyTorch version 1.6.0 available.\n",
      "I0821 14:41:09.233434 140736034558848 file_utils.py:55] TensorFlow version 2.2.0 available.\n",
      "I0821 14:41:10.136711 140736034558848 SentenceTransformer.py:31] Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "I0821 14:41:10.137421 140736034558848 SentenceTransformer.py:34] Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "I0821 14:41:10.139036 140736034558848 SentenceTransformer.py:55] Downloading sentence transformer model from https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/bert-base-nli-mean-tokens.zip and saving it at /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "100%|██████████| 405M/405M [11:46<00:00, 573kB/s]    \n",
      "I0821 14:53:03.760298 140736034558848 SentenceTransformer.py:69] Load SentenceTransformer from folder: /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "I0821 14:53:03.778604 140736034558848 configuration_utils.py:262] loading configuration file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json\n",
      "I0821 14:53:03.780069 140736034558848 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0821 14:53:03.783859 140736034558848 modeling_utils.py:665] loading weights file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin\n",
      "I0821 14:53:05.798384 140736034558848 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0821 14:53:05.893327 140736034558848 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "I0821 14:53:05.899291 140736034558848 tokenization_utils_base.py:1167] Model name '/Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0821 14:53:05.900619 140736034558848 tokenization_utils_base.py:1197] Didn't find file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "I0821 14:53:05.901818 140736034558848 tokenization_utils_base.py:1197] Didn't find file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer.json. We won't load it.\n",
      "I0821 14:53:05.908735 140736034558848 tokenization_utils_base.py:1252] loading file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt\n",
      "I0821 14:53:05.909595 140736034558848 tokenization_utils_base.py:1252] loading file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "I0821 14:53:05.910590 140736034558848 tokenization_utils_base.py:1252] loading file /Users/luoyonggui/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "I0821 14:53:05.911644 140736034558848 tokenization_utils_base.py:1252] loading file None\n",
      "I0821 14:53:05.912699 140736034558848 tokenization_utils_base.py:1252] loading file None\n",
      "I0821 14:53:06.001668 140736034558848 SentenceTransformer.py:90] Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0821 15:22:29.079473 140736034558848 SentenceTransformer.py:138] Start tokenization 3 sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef0613aa2aa4fc1978834ef9115e84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Then provide some sentences to the model.\n",
    "\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "sentence_embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: 768\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: 768\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: 768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And that's it already. We now have a list of numpy arrays with the embeddings.\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    \n",
    "    print(\"Embedding:\", embedding.size)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task.\n",
    "## Dataset Download\n",
    "\n",
    "First, you should download some datasets. For this run the examples/datasets/get_data.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training from Scratch\n",
    "\n",
    "training_nli.py fine-tunes BERT (and other transformer models) from the pre-trained model as provided by Google & Co. It tunes the model on Natural Language Inference (NLI) data. \n",
    "\n",
    "Given two sentences, the model should classify if these two sentence entail, contradict, or are neutral to each other. \n",
    "\n",
    "For this, the two sentences are passed to a transformer model to generate fixed-sized sentence embeddings. These sentence embeddings are then passed to a softmax classifier to derive the final label (entail, contradict, neutral). \n",
    "\n",
    "This generates sentence embeddings that are useful also for other tasks like clustering or semantic textual similarity.\n",
    "\n",
    "First, we define a sequential model of how a sentence is mapped to a fixed size sentence embedding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "We implemented various loss-functions that allow training of sentence embeddings from various datasets. These loss-functions are in the package sentence_transformers.losses.\n",
    "\n",
    "### SoftmaxLoss:   \n",
    "Given the sentence embeddings of two sentences, trains a softmax-classifier. Useful for training on datasets like NLI.\n",
    "### CosineSimilarityLoss:   \n",
    "Given a sentence pair and a gold similarity score (either between -1 and 1 or between 0 and 1), computes the cosine similarity between the sentence embeddings and minimizes the mean squared error loss.\n",
    "### TripletLoss:   \n",
    "Given a triplet (anchor, positive example, negative example), minimizes the triplet loss.\n",
    "### BatchHardTripletLoss:   \n",
    "Implements the batch hard triplet loss from the paper In Defense of the Triplet Loss for Person Re-Identification. Each batch must contain multiple examples from the same class. The loss optimizes then the distance between the most-distance positive pair and the closest negative-pair.\n",
    "### MultipleNegativesRankingLoss:   \n",
    "Each batch has one positive pair, all other pairs are treated as negative examples. The loss was used in the papers Efficient Natural Language Response Suggestion for Smart Reply and Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
