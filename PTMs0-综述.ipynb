{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTMs0-综述\n",
    "https://zhuanlan.zhihu.com/p/139015428\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文Pre-trained Models for Natural Language Processing: A Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"550\"\n",
       "            src=\"https://arxiv.org/pdf/2003.08271\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104185668>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://arxiv.org/pdf/2003.08271', width=1200, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era.  In this survey, we provide a comprehensive review of PTMs for NLP. \n",
    "\n",
    "We first briefly introduce language representation learning and itsresearch progress. \n",
    "\n",
    "Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. \n",
    "\n",
    "Next,we describe how to adapt the knowledge of PTMs to downstream tasks. \n",
    "\n",
    "Finally, we outline some potential directions of PTMs forfuture research. \n",
    "\n",
    "This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLPtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/ptm01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, substantial work has shown that pre-trained mod-els (PTMs), on the large corpus can learn universal languagerepresentations, which are beneficial for downstream NLPtasks and can avoid training a new model from scratch. Withthe development of computational power, the emergence ofthe deep models (i.e., Transformer [184]), and the constantenhancement of training skills, the architecture of PTMs hasbeen advanced from shallow to deep.  Thefirst-generationPTMsaim to learn good word embeddings. Since these mod-els themselves are no longer needed by downstream tasks, they are usually very shallow for computational efficiencies, such asSkip-Gram [129] and GloVe [133]. Although these pre-trainedembeddings can capture semantic meanings of words, they arecontext-free and fail to capture higher-level concepts in con-text, such as polysemous disambiguation, syntactic structures,semantic roles, anaphora. Thesecond-generation PTMsfocuson learning contextual word embeddings, such as CoVe [126],ELMo  [135],  OpenAI  GPT  [142]  and  BERT  [36].   Theselearned encoders are still needed to represent words in contextby downstream tasks. Besides, various pre-training tasks arealso proposed to learn PTMs for different purposes.The  contributions  of  this  survey  can  be  summarized  asfollows:\n",
    "\n",
    "Comprehensive review.We provide a comprehensivereview of PTMs for NLP, including background knowl-edge,  model architecture,  pre-training tasks,  variousextensions, adaption approaches, and applications.\n",
    "\n",
    "2.New taxonomy. We propose a taxonomy of PTMs forNLP, which categorizes existing PTMs from four dif-ferent perspectives:  1) representation type, 2) modelarchitecture; 3) type of pre-training task; 4) extensions for specific types of scenarios.\n",
    "\n",
    "3.Abundant resources.We collect abundant resourceson PTMs, including open-source implementations ofPTMs, visualization tools, corpora, and paper lists.\n",
    "\n",
    "4.Future directions.We discuss and analyze the limi-tations of existing PTMs.  Also, we suggest possiblefuture research directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
